# Anti-Bot Bypass Tools Research
**Research Date:** November 21, 2025  
**Purpose:** Identify and evaluate Python libraries for bypassing Cloudflare protection and bot detection

---

## Table of Contents
1. [cloudscraper](#1-cloudscraper)
2. [undetected-chromedriver](#2-undetected-chromedriver)
3. [playwright-stealth](#3-playwright-stealth)
4. [selenium-stealth](#4-selenium-stealth)
5. [curl_cffi](#5-curl_cffi)
6. [FlareSolverr](#6-flaresolverr)
7. [Comparison Matrix](#comparison-matrix)
8. [Integration Recommendations](#integration-recommendations)

---

## 1. cloudscraper

### Overview
Enhanced Python library for bypassing Cloudflare's anti-bot page (IUAM - "I'm Under Attack Mode"). Supports Cloudflare v1, v2, v3 JavaScript VM challenges, and Turnstile.

### GitHub Repository
**URL:** https://github.com/VeNoMouS/cloudscraper  
**Stars:** 5.8k ‚≠ê  
**Forks:** 588  
**License:** MIT  
**Latest Version:** v3.0.0 (Major upgrade - June 2024)

### Installation
```bash
pip install cloudscraper
```

### Key Features
- ‚úÖ **Automatic 403 Error Recovery** - Intelligent session refresh
- ‚úÖ **Session Health Monitoring** - Proactive session management
- ‚úÖ **Cloudflare v3 JavaScript VM Challenge Support** - Latest protection
- ‚úÖ **Turnstile Support** - Cloudflare's CAPTCHA alternative
- ‚úÖ **Proxy Rotation** - Smart rotation with multiple strategies
- ‚úÖ **Stealth Mode** - Human-like behavior simulation
- ‚úÖ **Multiple JavaScript Interpreters** - js2py (default), nodejs, native
- ‚úÖ **PyInstaller Compatible** - Works in compiled executables
- ‚úÖ **Python 3.8+** - Modern Python support

### Pros
- ‚úÖ **Requests-like API** - Easy migration from requests library
- ‚úÖ **100% Test Success Rate** - All core functionality tested
- ‚úÖ **Active Development** - Regular updates for new Cloudflare challenges
- ‚úÖ **No Browser Required** - Pure Python solution
- ‚úÖ **Fast** - Minimal overhead compared to browser automation
- ‚úÖ **Comprehensive Documentation** - Excellent examples and guides
- ‚úÖ **CAPTCHA Solver Integration** - Supports 2captcha, anticaptcha, etc.

### Cons
- ‚ùå **IP Reputation Matters** - Won't hide your datacenter IP
- ‚ùå **Cloudflare Updates** - May break when Cloudflare changes techniques
- ‚ùå **Limited to HTTP/HTTPS** - No WebSocket support
- ‚ùå **JavaScript Dependency** - Requires JS interpreter for challenges

### Basic Usage Example
```python
import cloudscraper

# Simple usage
scraper = cloudscraper.create_scraper()
response = scraper.get('https://example.com')
print(response.text)

# Advanced with all features
scraper = cloudscraper.create_scraper(
    interpreter='js2py',  # Best compatibility for v3 challenges
    delay=5,              # Extra time for complex challenges
    enable_stealth=True,  # Human-like behavior
    stealth_options={
        'min_delay': 2.0,
        'max_delay': 6.0,
        'human_like_delays': True,
        'randomize_headers': True,
        'browser_quirks': True
    },
    browser='chrome',
    debug=True
)
response = scraper.get('https://example.com')
```

### Proxy Rotation Example
```python
proxies = [
    'http://user:pass@proxy1.example.com:8080',
    'http://user:pass@proxy2.example.com:8080',
    'http://user:pass@proxy3.example.com:8080'
]

scraper = cloudscraper.create_scraper(
    rotating_proxies=proxies,
    proxy_options={
        'rotation_strategy': 'smart',  # or 'sequential', 'random'
        'ban_time': 300  # seconds to ban failed proxy
    },
    enable_stealth=True
)

for i in range(5):
    response = scraper.get('https://example.com')
    print(f"Request {i+1}: {response.status_code}")
```

### CAPTCHA Solving Example
```python
# Using 2captcha for Turnstile challenges
scraper = cloudscraper.create_scraper(
    captcha={
        'provider': '2captcha',
        'api_key': 'your_2captcha_api_key'
    },
    debug=True
)

response = scraper.get('https://turnstile-protected-site.com')
```

### Integration with Our Project
```python
# In utils/http_client.py
import cloudscraper

class CloudflareBypassClient:
    def __init__(self):
        self.scraper = cloudscraper.create_scraper(
            interpreter='js2py',
            delay=5,
            enable_stealth=True,
            stealth_options={
                'min_delay': 2.0,
                'max_delay': 6.0,
                'human_like_delays': True,
                'randomize_headers': True,
                'browser_quirks': True
            },
            browser='chrome'
        )
    
    def get(self, url, **kwargs):
        try:
            response = self.scraper.get(url, **kwargs)
            return response
        except Exception as e:
            print(f"Error with cloudscraper: {e}")
            # Fallback to other methods
            return None
```

---

## 2. undetected-chromedriver

### Overview
Optimized Selenium ChromeDriver patch that bypasses anti-bot services like Distill Network, Imperva, DataDome, and Cloudflare. Automatically patches the driver binary.

### GitHub Repository
**URL:** https://github.com/ultrafunkamsterdam/undetected-chromedriver  
**Stars:** 12k ‚≠ê  
**Forks:** 1.3k  
**License:** GPL-3.0  
**Language:** Python 100%

### Installation
```bash
pip install undetected-chromedriver

# Or from GitHub
pip install git+https://github.com/ultrafunkamsterdam/undetected-chromedriver@master
```

### Key Features
- ‚úÖ **Anti-Detection Logic** - Prevents detection variables from being injected
- ‚úÖ **Automatic Driver Management** - Downloads and patches ChromeDriver
- ‚úÖ **Chrome DevTools Protocol (CDP)** - Low-level event listeners
- ‚úÖ **Headless Support** - Now undetected in headless mode (as of v3.4.5)
- ‚úÖ **Chrome/Chromium/Brave Support** - Works with multiple browsers
- ‚úÖ **No Manual Driver Updates** - Auto-downloads correct version
- ‚úÖ **Python 3.6+** - Wide compatibility

### Pros
- ‚úÖ **Passes All Bot Tests** - Including bot.sannysoft.com, nowsecure.nl
- ‚úÖ **Real Browser** - Full JavaScript execution
- ‚úÖ **Google Account Login** - Can maintain normal reCAPTCHA v3 scores
- ‚úÖ **Active Maintenance** - Regular updates for Chrome versions
- ‚úÖ **Large Community** - 10.8k+ projects using it
- ‚úÖ **CDP Integration** - Advanced debugging capabilities

### Cons
- ‚ùå **Memory Heavy** - Each browser instance uses significant RAM
- ‚ùå **Slower** - Browser automation is inherently slower
- ‚ùå **IP Detection** - Won't hide datacenter IPs
- ‚ùå **Headless Still WIP** - Officially unsupported (but works)
- ‚ùå **Chrome Dependency** - Requires Chrome/Chromium installation

### Basic Usage Example
```python
import undetected_chromedriver as uc

# Simple usage
driver = uc.Chrome()
driver.get('https://nowsecure.nl')
driver.save_screenshot('screenshot.png')
driver.quit()

# With options
options = uc.ChromeOptions()
options.add_argument("--start-maximized")
# options.add_argument("--headless")  # Headless mode

driver = uc.Chrome(options=options)
driver.get('https://example.com')
```

### Advanced Usage with Profile
```python
import undetected_chromedriver as uc

options = uc.ChromeOptions()
# Use specific profile folder
options.user_data_dir = "c:\\temp\\profile"

# Use specific Chrome version
driver = uc.Chrome(options=options, version_main=94)
driver.get('https://nowsecure.nl')
```

### CDP Event Listeners Example
```python
import undetected_chromedriver as uc
from pprint import pformat

driver = uc.Chrome(enable_cdp_events=True)

def print_network_data(eventdata):
    print(pformat(eventdata))

# Listen to network events
driver.add_cdp_listener("Network.dataReceived", print_network_data)
driver.add_cdp_listener("Network.requestWillBeSent", print_network_data)

# Wildcard captures all events
# driver.add_cdp_listener('*', print_network_data)

driver.get('https://nowsecure.nl')
```

### Integration with Our Project
```python
# In scrapers/base_scraper.py
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class UndetectedBrowserScraper:
    def __init__(self, headless=False):
        options = uc.ChromeOptions()
        if headless:
            options.add_argument("--headless")
        options.add_argument("--disable-blink-features=AutomationControlled")
        
        self.driver = uc.Chrome(options=options)
    
    def get_page(self, url, wait_for_selector=None):
        try:
            self.driver.get(url)
            
            if wait_for_selector:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, wait_for_selector))
                )
            
            return self.driver.page_source
        except Exception as e:
            print(f"Error: {e}")
            return None
    
    def close(self):
        self.driver.quit()
```

---

## 3. playwright-stealth

### Overview
Python port of puppeteer-extra-plugin-stealth for Playwright. Adds stealth mode to bypass bot detection systems.

### GitHub Repository
**URL:** https://github.com/AtuboDad/playwright_stealth  
**Stars:** 837 ‚≠ê  
**Forks:** 109  
**License:** MIT  
**Language:** Python 100%

### Installation
```bash
pip install playwright-stealth
```

### Key Features
- ‚úÖ **Playwright Integration** - Works with both sync and async Playwright
- ‚úÖ **Stealth Evasions** - Multiple detection bypass techniques
- ‚úÖ **Cross-Browser** - Chrome, Firefox, WebKit support
- ‚úÖ **Simple API** - Single function call to enable
- ‚úÖ **Lightweight** - Minimal overhead

### Pros
- ‚úÖ **Easy Integration** - One-liner to add stealth
- ‚úÖ **Playwright Benefits** - Fast, modern browser automation
- ‚úÖ **Multiple Browsers** - Not limited to Chrome
- ‚úÖ **Async Support** - Perfect for concurrent scraping
- ‚úÖ **Active** - Used by 1k+ projects

### Cons
- ‚ùå **Not Perfect** - Author admits it's "Not perfect"
- ‚ùå **Limited Documentation** - Basic examples only
- ‚ùå **Playwright Required** - Need to learn Playwright API
- ‚ùå **Less Comprehensive** - Fewer features than cloudscraper

### Synchronous Usage Example
```python
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync

with sync_playwright() as p:
    browser = p.chromium.launch()
    page = browser.new_page()
    stealth_sync(page)
    page.goto('https://bot.sannysoft.com')
    page.screenshot(path='stealth-test.png')
    browser.close()
```

### Asynchronous Usage Example
```python
import asyncio
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await stealth_async(page)
        await page.goto('https://bot.sannysoft.com')
        await page.screenshot(path='stealth-test.png')
        await browser.close()

asyncio.run(main())
```

### Multiple URLs Concurrently
```python
import asyncio
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

async def scrape_url(page, url):
    await page.goto(url)
    return await page.content()

async def main():
    urls = [
        "https://google.com/",
        "https://facebook.com/",
        "https://twitter.com/",
    ]
    
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await stealth_async(page)
        
        tasks = [scrape_url(page, url) for url in urls]
        results = await asyncio.gather(*tasks)
        
        await browser.close()
        return results

asyncio.run(main())
```

### Integration with Our Project
```python
# In scrapers/playwright_scraper.py
import asyncio
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

class PlaywrightStealthScraper:
    def __init__(self):
        self.playwright = None
        self.browser = None
    
    async def __aenter__(self):
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(headless=True)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.browser.close()
        await self.playwright.stop()
    
    async def scrape(self, url):
        page = await self.browser.new_page()
        await stealth_async(page)
        await page.goto(url)
        content = await page.content()
        await page.close()
        return content

# Usage
async def main():
    async with PlaywrightStealthScraper() as scraper:
        html = await scraper.scrape('https://example.com')
        print(html)

asyncio.run(main())
```

---

## 4. selenium-stealth

### Overview
Python package to prevent Selenium detection by making it more stealthy. Implements techniques similar to puppeteer-extra-plugin-stealth for Selenium.

### GitHub Repository
**URL:** https://github.com/diprajpatra/selenium-stealth  
**Stars:** 733 ‚≠ê  
**Forks:** 98  
**License:** MIT  
**Language:** Python 100%

### Installation
```bash
pip install selenium-stealth
```

### Key Features
- ‚úÖ **Selenium Integration** - Works with standard Selenium
- ‚úÖ **Chrome/Chromium Support** - Designed for Chrome
- ‚úÖ **Multiple Evasions** - WebGL, navigator, plugins, etc.
- ‚úÖ **Simple API** - Easy to integrate
- ‚úÖ **Customizable** - Many parameters to tweak

### Pros
- ‚úÖ **Passes Bot Tests** - bot.sannysoft.com, browserleaks.com
- ‚úÖ **Google Login Support** - Can maintain normal reCAPTCHA scores
- ‚úÖ **Standard Selenium** - No special ChromeDriver needed
- ‚úÖ **Easy to Use** - Simple function call
- ‚úÖ **Well Documented** - Clear examples

### Cons
- ‚ùå **Chrome Only** - Limited to Chrome/Chromium
- ‚ùå **Less Active** - Fewer recent updates
- ‚ùå **Headless Detection** - Headless mode still detectable
- ‚ùå **Manual Driver Management** - Need to manage ChromeDriver yourself

### Basic Usage Example
```python
from selenium import webdriver
from selenium_stealth import stealth
import time

options = webdriver.ChromeOptions()
options.add_argument("start-maximized")
options.add_experimental_option("excludeSwitches", ["enable-automation"])
options.add_experimental_option('useAutomationExtension', False)

driver = webdriver.Chrome(options=options)

stealth(driver,
    languages=["en-US", "en"],
    vendor="Google Inc.",
    platform="Win32",
    webgl_vendor="Intel Inc.",
    renderer="Intel Iris OpenGL Engine",
    fix_hairline=True,
)

driver.get("https://bot.sannysoft.com/")
time.sleep(5)
driver.quit()
```

### Customized Stealth Parameters
```python
from selenium import webdriver
from selenium_stealth import stealth

driver = webdriver.Chrome()

stealth(
    driver,
    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.53 Safari/537.36',
    languages=["en-US", "en"],
    vendor="Google Inc.",
    platform="Win32",
    webgl_vendor="Intel Inc.",
    renderer="Intel Iris OpenGL Engine",
    fix_hairline=False,
    run_on_insecure_origins=False,
)

driver.get("https://example.com")
```

### Integration with Our Project
```python
# In scrapers/selenium_scraper.py
from selenium import webdriver
from selenium_stealth import stealth

class SeleniumStealthScraper:
    def __init__(self):
        options = webdriver.ChromeOptions()
        options.add_argument("start-maximized")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        self.driver = webdriver.Chrome(options=options)
        
        stealth(self.driver,
            languages=["en-US", "en"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
        )
    
    def scrape(self, url):
        self.driver.get(url)
        return self.driver.page_source
    
    def close(self):
        self.driver.quit()
```

---

## 5. curl_cffi

### Overview
Python binding for curl-impersonate that can impersonate browsers' TLS/JA3 and HTTP/2 fingerprints. Fast alternative to pure Python HTTP clients.

### GitHub Repository
**URL:** https://github.com/yifeikong/curl_cffi  
**Stars:** 4.5k ‚≠ê  
**Forks:** 421  
**License:** MIT  
**Language:** Python 98.4%

### Installation
```bash
pip install curl_cffi --upgrade

# For beta releases
pip install curl_cffi --upgrade --pre
```

### Key Features
- ‚úÖ **JA3/TLS Fingerprint Impersonation** - Mimics real browsers
- ‚úÖ **HTTP/2 & HTTP/3 Support** - Modern protocols
- ‚úÖ **Fast** - On par with aiohttp/pycurl
- ‚úÖ **Requests-like API** - Easy to use
- ‚úÖ **AsyncIO Support** - Concurrent requests
- ‚úÖ **WebSocket Support** - Full WebSocket implementation
- ‚úÖ **Pre-compiled** - No compilation needed

### Supported Browser Fingerprints
- Chrome (99, 100, 101, 104, 107, 110, 116, 119, 120, 123, 124, 131, 133a, 136)
- Chrome Android (99, 131)
- Safari (153, 155, 170, 180, 184, 260)
- Safari iOS (172, 180, 184, 260)
- Firefox (133, 135)
- Edge (99, 101)

### Pros
- ‚úÖ **Fastest** - Much faster than requests/httpx
- ‚úÖ **Modern** - HTTP/2, HTTP/3, WebSocket support
- ‚úÖ **Accurate Fingerprints** - Real browser TLS signatures
- ‚úÖ **No Browser** - Lightweight, no browser needed
- ‚úÖ **Active Development** - Regular updates
- ‚úÖ **Commercial Support** - impersonate.pro available

### Cons
- ‚ùå **Limited to HTTP** - No full browser features
- ‚ùå **Fingerprint Updates** - Need library updates for new browsers
- ‚ùå **No JavaScript** - Can't execute JS challenges
- ‚ùå **Learning Curve** - Some curl-specific concepts

### Basic Usage Example
```python
import curl_cffi

# Simple impersonation
r = curl_cffi.get("https://tls.browserleaks.com/json", impersonate="chrome")
print(r.json())

# Auto-update to latest Chrome
r = curl_cffi.get("https://example.com", impersonate="chrome")

# Pin specific version
r = curl_cffi.get("https://example.com", impersonate="chrome124")

# Custom JA3/Akamai strings
r = curl_cffi.get("https://example.com", ja3="...", akamai="...")
```

### Session Example
```python
import curl_cffi

s = curl_cffi.Session()

# Cookies are maintained
s.get("https://httpbin.org/cookies/set/foo/bar")
print(s.cookies)

r = s.get("https://httpbin.org/cookies")
print(r.json())  # {'cookies': {'foo': 'bar'}}
```

### Async Example
```python
import asyncio
from curl_cffi import AsyncSession

async def main():
    urls = [
        "https://google.com/",
        "https://facebook.com/",
        "https://twitter.com/",
    ]
    
    async with AsyncSession() as s:
        tasks = [s.get(url, impersonate="chrome") for url in urls]
        results = await asyncio.gather(*tasks)
        
        for r in results:
            print(f"{r.url}: {r.status_code}")

asyncio.run(main())
```

### WebSocket Example
```python
from curl_cffi import WebSocket

def on_message(ws: WebSocket, message: str | bytes):
    print(message)

ws = WebSocket(on_message=on_message)
ws.run_forever("wss://api.gemini.com/v1/marketdata/BTCUSD")
```

### Integration with Our Project
```python
# In utils/curl_client.py
import curl_cffi
from curl_cffi import AsyncSession
import asyncio

class CurlCffiClient:
    def __init__(self, impersonate="chrome"):
        self.session = curl_cffi.Session()
        self.impersonate = impersonate
    
    def get(self, url, **kwargs):
        try:
            response = self.session.get(
                url, 
                impersonate=self.impersonate,
                **kwargs
            )
            return response
        except Exception as e:
            print(f"Error: {e}")
            return None

class AsyncCurlClient:
    def __init__(self, impersonate="chrome"):
        self.impersonate = impersonate
    
    async def scrape_urls(self, urls):
        async with AsyncSession() as s:
            tasks = [
                s.get(url, impersonate=self.impersonate) 
                for url in urls
            ]
            return await asyncio.gather(*tasks)
```

---

## 6. FlareSolverr

### Overview
Proxy server to bypass Cloudflare and DDoS-GUARD protection. Uses Selenium with undetected-chromedriver to solve challenges.

### GitHub Repository
**URL:** https://github.com/FlareSolverr/FlareSolverr  
**Stars:** 11.5k ‚≠ê  
**Forks:** 929  
**License:** MIT  
**Language:** Python 73.1%

### Installation

#### Docker (Recommended)
```bash
docker run -d \
  --name=flaresolverr \
  -p 8191:8191 \
  -e LOG_LEVEL=info \
  --restart unless-stopped \
  ghcr.io/flaresolverr/flaresolverr:latest
```

#### From Source
```bash
# Install Python 3.14 and Chrome/Chromium
pip install -r requirements.txt
python src/flaresolverr.py
```

### Key Features
- ‚úÖ **Proxy Server** - RESTful API for challenges
- ‚úÖ **Session Management** - Persistent browser sessions
- ‚úÖ **Cookie Extraction** - Returns cookies and HTML
- ‚úÖ **Docker Support** - Easy deployment
- ‚úÖ **Multiple Architectures** - x86, x86-64, ARM32, ARM64
- ‚úÖ **CAPTCHA Solver Integration** - Various providers supported

### Pros
- ‚úÖ **Language Agnostic** - Works with any HTTP client
- ‚úÖ **Centralized** - One service for multiple apps
- ‚úÖ **Session Reuse** - Efficient for multiple requests
- ‚úÖ **Well Documented** - Clear API documentation
- ‚úÖ **Active Project** - 11.5k stars, regular updates

### Cons
- ‚ùå **Resource Heavy** - Each request launches a browser
- ‚ùå **Additional Service** - Need to run separate server
- ‚ùå **Network Dependency** - Requires HTTP calls
- ‚ùå **CAPTCHA Issues** - Solvers currently not working

### API Usage Examples

#### Python - request.get
```python
import requests

url = "http://localhost:8191/v1"
headers = {"Content-Type": "application/json"}
data = {
    "cmd": "request.get",
    "url": "https://www.google.com/",
    "maxTimeout": 60000
}

response = requests.post(url, headers=headers, json=data)
result = response.json()

# Extract cookies and user agent
cookies = result["solution"]["cookies"]
user_agent = result["solution"]["userAgent"]
html = result["solution"]["response"]
```

#### Session Management
```python
import requests

base_url = "http://localhost:8191/v1"
headers = {"Content-Type": "application/json"}

# Create session
create_data = {
    "cmd": "sessions.create",
    "session": "my_session_id"
}
requests.post(base_url, headers=headers, json=create_data)

# Use session
request_data = {
    "cmd": "request.get",
    "url": "https://example.com",
    "session": "my_session_id"
}
response = requests.post(base_url, headers=headers, json=request_data)

# Destroy session when done
destroy_data = {
    "cmd": "sessions.destroy",
    "session": "my_session_id"
}
requests.post(base_url, headers=headers, json=destroy_data)
```

#### With Proxy
```python
data = {
    "cmd": "request.get",
    "url": "https://example.com/",
    "proxy": {
        "url": "http://127.0.0.1:8888",
        "username": "user",  # optional
        "password": "pass"   # optional
    },
    "maxTimeout": 60000
}
```

### Integration with Our Project
```python
# In utils/flaresolverr_client.py
import requests
import time

class FlareSolverrClient:
    def __init__(self, api_url="http://localhost:8191/v1"):
        self.api_url = api_url
        self.headers = {"Content-Type": "application/json"}
        self.sessions = {}
    
    def create_session(self, session_id=None):
        """Create persistent session for efficiency"""
        data = {"cmd": "sessions.create"}
        if session_id:
            data["session"] = session_id
        
        response = requests.post(self.api_url, headers=self.headers, json=data)
        result = response.json()
        
        if result["status"] == "ok":
            session_id = result.get("session", session_id)
            self.sessions[session_id] = time.time()
            return session_id
        return None
    
    def get_with_session(self, url, session_id):
        """Make request using existing session"""
        data = {
            "cmd": "request.get",
            "url": url,
            "session": session_id,
            "maxTimeout": 60000
        }
        
        response = requests.post(self.api_url, headers=self.headers, json=data)
        return response.json()
    
    def destroy_session(self, session_id):
        """Clean up session"""
        data = {
            "cmd": "sessions.destroy",
            "session": session_id
        }
        requests.post(self.api_url, headers=self.headers, json=data)
        if session_id in self.sessions:
            del self.sessions[session_id]
    
    def get(self, url, use_session=True):
        """Simple get request"""
        if use_session:
            session_id = f"session_{int(time.time())}"
            self.create_session(session_id)
            try:
                result = self.get_with_session(url, session_id)
                return result
            finally:
                self.destroy_session(session_id)
        else:
            data = {
                "cmd": "request.get",
                "url": url,
                "maxTimeout": 60000
            }
            response = requests.post(self.api_url, headers=self.headers, json=data)
            return response.json()

# Usage in scraper
def scrape_with_flaresolverr(url):
    client = FlareSolverrClient()
    result = client.get(url)
    
    if result["status"] == "ok":
        html = result["solution"]["response"]
        cookies = result["solution"]["cookies"]
        user_agent = result["solution"]["userAgent"]
        return html, cookies, user_agent
    return None, None, None
```

---

## Comparison Matrix

| Feature | cloudscraper | undetected-chromedriver | playwright-stealth | selenium-stealth | curl_cffi | FlareSolverr |
|---------|-------------|------------------------|-------------------|-----------------|-----------|--------------|
| **GitHub Stars** | 5.8k | 12k | 837 | 733 | 4.5k | 11.5k |
| **Ease of Use** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Speed** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **Memory Usage** | Low | High | Medium | High | Low | High |
| **Browser Required** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ |
| **JavaScript Execution** | Limited | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ |
| **Cloudflare v3** | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ùå | ‚úÖ |
| **Turnstile** | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ùå | ‚úÖ |
| **HTTP/2** | ‚ùå | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **HTTP/3** | ‚ùå | ‚ùå | ‚ùå | ‚ùå | ‚úÖ | ‚ùå |
| **AsyncIO** | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå |
| **WebSocket** | ‚ùå | ‚ùå | ‚úÖ | ‚ùå | ‚úÖ | ‚ùå |
| **Proxy Support** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **Stealth Mode** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **CAPTCHA Solving** | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ö†Ô∏è | ‚ùå | ‚ö†Ô∏è |
| **Session Management** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |
| **License** | MIT | GPL-3.0 | MIT | MIT | MIT | MIT |
| **Best For** | HTTP APIs | Full browser | Modern async | Selenium users | Speed | Service |

**Legend:**
- ‚úÖ Fully supported
- ‚ö†Ô∏è Partially supported or requires additional work
- ‚ùå Not supported

---

## Integration Recommendations

### 1. **Layered Approach** (Recommended)

Use multiple tools in a fallback chain for maximum reliability:

```python
# In utils/smart_client.py
import cloudscraper
import undetected_chromedriver as uc
from curl_cffi import Session as CurlSession
import requests

class SmartHTTPClient:
    """
    Intelligent HTTP client that tries multiple methods
    in order of speed and resource usage
    """
    
    def __init__(self):
        # Method 1: curl_cffi (fastest, TLS fingerprinting)
        self.curl_session = CurlSession()
        
        # Method 2: cloudscraper (fast, Cloudflare specific)
        self.cloudscraper = cloudscraper.create_scraper(
            interpreter='js2py',
            enable_stealth=True
        )
        
        # Method 3: undetected-chromedriver (slowest, most powerful)
        self.uc_driver = None
    
    def get(self, url, **kwargs):
        """Try methods in order until one works"""
        
        # Try curl_cffi first (fastest)
        try:
            print("Trying curl_cffi...")
            response = self.curl_session.get(url, impersonate="chrome", **kwargs)
            if response.status_code == 200:
                return response
        except Exception as e:
            print(f"curl_cffi failed: {e}")
        
        # Try cloudscraper (good for Cloudflare)
        try:
            print("Trying cloudscraper...")
            response = self.cloudscraper.get(url, **kwargs)
            if response.status_code == 200:
                return response
        except Exception as e:
            print(f"cloudscraper failed: {e}")
        
        # Last resort: undetected-chromedriver
        try:
            print("Trying undetected-chromedriver...")
            if self.uc_driver is None:
                options = uc.ChromeOptions()
                options.add_argument("--headless")
                self.uc_driver = uc.Chrome(options=options)
            
            self.uc_driver.get(url)
            html = self.uc_driver.page_source
            
            # Return fake response object
            class FakeResponse:
                def __init__(self, html):
                    self.text = html
                    self.content = html.encode()
                    self.status_code = 200
            
            return FakeResponse(html)
        except Exception as e:
            print(f"undetected-chromedriver failed: {e}")
        
        return None
    
    def close(self):
        """Clean up resources"""
        if self.uc_driver:
            self.uc_driver.quit()
```

### 2. **For Simple HTTP Requests**

**Recommended:** `curl_cffi`
- Fast, lightweight
- Good TLS fingerprinting
- Requests-like API

```python
import curl_cffi

session = curl_cffi.Session()
response = session.get("https://example.com", impersonate="chrome")
```

### 3. **For Cloudflare-Protected Sites**

**Recommended:** `cloudscraper` (primary) + `undetected-chromedriver` (fallback)

```python
import cloudscraper
import undetected_chromedriver as uc

# Try cloudscraper first
try:
    scraper = cloudscraper.create_scraper(
        interpreter='js2py',
        enable_stealth=True
    )
    response = scraper.get(url)
except:
    # Fallback to browser
    driver = uc.Chrome(headless=True)
    driver.get(url)
    html = driver.page_source
    driver.quit()
```

### 4. **For Complex JavaScript Sites**

**Recommended:** `playwright-stealth` (async) or `undetected-chromedriver` (sync)

```python
# Async approach with Playwright
import asyncio
from playwright.async_api import async_playwright
from playwright_stealth import stealth_async

async def scrape():
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await stealth_async(page)
        await page.goto(url)
        html = await page.content()
        await browser.close()
        return html
```

### 5. **For Microservices Architecture**

**Recommended:** `FlareSolverr`
- Centralized service
- Language-agnostic
- Good for multiple apps

```python
# Start FlareSolverr as Docker container
# Then use simple HTTP requests
import requests

response = requests.post(
    "http://localhost:8191/v1",
    json={
        "cmd": "request.get",
        "url": "https://example.com",
        "maxTimeout": 60000
    }
)
```

### 6. **Recommended for Our Universal Scraper Project**

Based on our needs, I recommend implementing:

1. **Primary:** `curl_cffi` - Fast, modern, good TLS fingerprinting
2. **Secondary:** `cloudscraper` - Excellent Cloudflare bypass
3. **Tertiary:** `undetected-chromedriver` - Last resort for tough sites
4. **Optional:** `FlareSolverr` - For centralized challenge solving

#### Implementation Structure

```
utils/
‚îú‚îÄ‚îÄ http_client.py          # Base HTTP client interface
‚îú‚îÄ‚îÄ curl_client.py          # curl_cffi implementation
‚îú‚îÄ‚îÄ cloudflare_client.py    # cloudscraper implementation
‚îú‚îÄ‚îÄ browser_client.py       # undetected-chromedriver implementation
‚îú‚îÄ‚îÄ flaresolverr_client.py  # FlareSolverr integration
‚îî‚îÄ‚îÄ smart_client.py         # Intelligent fallback client

scrapers/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ base_scraper.py         # Base scraper with smart_client
‚îú‚îÄ‚îÄ gsmarena.py             # Uses appropriate client
‚îú‚îÄ‚îÄ kimovil.py              # Uses appropriate client
‚îî‚îÄ‚îÄ mobiles91.py            # Uses appropriate client
```

---

## Practical Integration Steps

### Step 1: Install Dependencies
```bash
# Install all recommended tools
pip install curl_cffi --upgrade
pip install cloudscraper --upgrade
pip install undetected-chromedriver
pip install requests
```

### Step 2: Update requirements.txt
```txt
# Add to requirements.txt
curl_cffi>=0.7.0
cloudscraper>=3.0.0
undetected-chromedriver>=3.5.0
requests>=2.31.0
selenium>=4.10.0
```

### Step 3: Create Smart Client
Create `utils/smart_client.py` as shown in recommendations above.

### Step 4: Update Scrapers
Update each scraper to use the smart client:

```python
# In scrapers/gsmarena.py
from utils.smart_client import SmartHTTPClient

class GSMArenaScraper:
    def __init__(self):
        self.client = SmartHTTPClient()
    
    def scrape(self, url):
        response = self.client.get(url)
        if response:
            return self.parse(response.text)
        return None
    
    def close(self):
        self.client.close()
```

### Step 5: Add Configuration
```python
# In config.py
class ScraperConfig:
    # HTTP Client preferences
    USE_CURL_CFFI = True
    USE_CLOUDSCRAPER = True
    USE_BROWSER = False  # Only if needed
    
    # Browser settings
    BROWSER_HEADLESS = True
    BROWSER_TIMEOUT = 30
    
    # FlareSolverr (optional)
    FLARESOLVERR_ENABLED = False
    FLARESOLVERR_URL = "http://localhost:8191/v1"
    
    # Retry settings
    MAX_RETRIES = 3
    RETRY_DELAY = 2
```

---

## Testing & Verification

### Test Sites
1. **https://bot.sannysoft.com/** - Comprehensive bot detection test
2. **https://nowsecure.nl/** - Cloudflare challenge test
3. **https://tls.browserleaks.com/json** - TLS fingerprint verification
4. **https://www.httpbin.org/** - HTTP testing

### Test Script
```python
# test_anti_bot.py
from utils.smart_client import SmartHTTPClient

def test_bot_detection():
    client = SmartHTTPClient()
    
    test_urls = [
        "https://bot.sannysoft.com/",
        "https://tls.browserleaks.com/json",
        "https://www.httpbin.org/user-agent",
    ]
    
    for url in test_urls:
        print(f"\nTesting: {url}")
        response = client.get(url)
        if response:
            print(f"‚úÖ Success: {response.status_code}")
            print(f"Length: {len(response.text)} chars")
        else:
            print(f"‚ùå Failed")
    
    client.close()

if __name__ == "__main__":
    test_bot_detection()
```

---

## Cost-Benefit Analysis

### curl_cffi
- **Setup Cost:** Low (just pip install)
- **Runtime Cost:** Very low (fast, lightweight)
- **Maintenance:** Low (stable API)
- **Success Rate:** High for TLS-based detection

### cloudscraper  
- **Setup Cost:** Low (pip install)
- **Runtime Cost:** Low-Medium (JS interpreter overhead)
- **Maintenance:** Medium (Cloudflare updates)
- **Success Rate:** Very high for Cloudflare

### undetected-chromedriver
- **Setup Cost:** Medium (Chrome installation)
- **Runtime Cost:** High (memory + CPU)
- **Maintenance:** Medium (Chrome version updates)
- **Success Rate:** Very high for all detection

### FlareSolverr
- **Setup Cost:** High (Docker setup, separate service)
- **Runtime Cost:** High (browser per request)
- **Maintenance:** Low (runs as service)
- **Success Rate:** Very high for Cloudflare

---

## Conclusion

For the **Universal Scraper project**, I recommend:

1. **Start with `curl_cffi`** - Fast, lightweight, good fingerprinting
2. **Add `cloudscraper`** - Excellent Cloudflare bypass, moderate overhead
3. **Keep `undetected-chromedriver`** - Nuclear option for tough sites
4. **Consider `FlareSolverr`** - If scaling to multiple apps

This layered approach provides:
- ‚ö° Speed when possible (curl_cffi)
- üõ°Ô∏è Cloudflare protection (cloudscraper)
- üí™ Full browser power when needed (undetected-chromedriver)
- üìà Scalability option (FlareSolverr)

The smart client implementation will automatically choose the best tool for each situation, optimizing for speed while maintaining high success rates against bot detection.

---

**Document Version:** 1.0  
**Last Updated:** November 21, 2025  
**Author:** GitHub Copilot (AI Assistant)
