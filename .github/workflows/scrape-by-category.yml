name: Scrape By Category

on:
  workflow_dispatch:
    inputs:
      max_results_per_category:
        description: 'Max results per category (0 for all)'
        required: false
        default: '100'
      delay_between_categories:
        description: 'Delay between categories in seconds'
        required: false
        default: '15'

jobs:
  scrape-by-category:
    runs-on: ubuntu-latest
    timeout-minutes: 720  # 12 hours max
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          clean: true
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'
      
      - name: Clear Python cache
        run: |
          pip cache purge || true
          find . -type d -name __pycache__ -exec rm -rf {} + || true
          find . -type f -name "*.pyc" -delete || true
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps
      
      - name: Run category scraper
        env:
          MONGO_DB_USERNAME: ${{ secrets.MONGO_DB_USERNAME }}
          MONGO_DB_PASSWORD: ${{ secrets.MONGO_DB_PASSWORD }}
          MONGO_DB_DATABASE_NAME: ${{ secrets.MONGO_DB_DATABASE_NAME }}
          MONGO_DB_DOMAIN_NAME: ${{ secrets.MONGO_DB_DOMAIN_NAME }}
          MAX_RESULTS_PER_CATEGORY: ${{ github.event.inputs.max_results_per_category || '100' }}
          DELAY_BETWEEN_CATEGORIES: ${{ github.event.inputs.delay_between_categories || '15' }}
        run: |
          python scrape_by_category.py
      
      - name: Upload summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: category-scrape-summary
          path: |
            data/*.json
          retention-days: 30
