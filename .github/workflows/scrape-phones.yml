name: Scrape Phone Data

on:
  # Run on manual trigger
  workflow_dispatch:
    inputs:
      search_query:
        description: 'Search query (e.g., Samsung S24)'
        required: true
        default: 'Samsung'
      max_results:
        description: 'Maximum results to scrape (0 for all)'
        required: false
        default: '20'
  
  # Run on schedule (optional - uncomment to enable)
  # schedule:
  #   - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps
      
      - name: Run scraper with headless browser
        env:
          MONGO_DB_USERNAME: "baeonuser"
          MONGO_DB_PASSWORD: "6JtLUmRHTMZ8IYJA"
          MONGO_DB_DATABASE_NAME: "baeonDBStage"
          MONGO_DB_DOMAIN_NAME: "baeonncluster.oakjn89"
          SEARCH_QUERY: ${{ github.event.inputs.search_query || 'Samsung' }}
          MAX_RESULTS: ${{ github.event.inputs.max_results || '20' }}
        run: |
          python github_scraper.py
      
      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results
          path: data/*.json
          retention-days: 30
      
      - name: Commit results (optional)
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add data/*.json || true
          git diff --quiet && git diff --staged --quiet || git commit -m "Update scraped data - $(date +'%Y-%m-%d %H:%M:%S')"
          git push || true
        continue-on-error: true
